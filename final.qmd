---
title: "BSMM-final"
subtitle: "BSMM 8740 Fall 2023"
author: "Add your name here"
date: "Add the date here"
editor: visual
format: html
self-contained: true
---

```{r}
#| echo: false
require(magrittr, quietly=T)
require(ggplot2, quietly=T)
```

# Instructions

::: callout-important
To access Github from the lab, you will need to make sure you are logged in as follows:

-   username: **.\\daladmin**
-   password: **Business507!**

Remember to

-   create your PAT using `usethis::create_github_token()` ,
-   store your PAT with `gitcreds::gitcreds_set()` ,
-   set your username and email with
    -   `usethis::use_git_config( user.name = ___, user.email = ___)`
:::

## Overview

The midterm will be released on Monday, October 30, and is designed to be completed in 60+ minutes.

The exam will consist of two parts:

1.  **Part 1 - Conceptual:** Simple questions designed to evaluate your familiarity with the written course notes.
2.  **Part 2 - Applied:** Data analysis in RStudio (like a usual lab, but simpler).

Log in to *your* github account and then go to the [GitHub organization](https://github.com/bsmm-8740-fall-2023) for the course and find the **BSMM-midterm-\[your github username\]** repository to complete the exam.

Create an R project using your `midterm` repository (remember to create a PAT, etc., as in lab-1) and add your answers by editing the `midterm.qmd` file in your repository. Your first edits should be to the **date** and **your name** (as author) at the top of this document.

Be sure that you have [saved]{.underline}, [staged]{.underline}, [committed]{.underline}, and [pushed]{.underline} your work before the end of the exam.

ðŸ€ Good luck! ðŸ€

## Academic Integrity

By taking this exam, you pledge to that:

-   I will not lie, cheat, or steal in my academic endeavors;

-   I will conduct myself honorably in all my endeavors; and

-   I will act if the Standard is compromised.

## Rules & Notes

-   This is an individual assignment. Everything in your repository is for your eyes only.

-   You may not collaborate or communicate anything about this exam to **anyone** except the instructor. For example, you may not communicate with other students or post/solicit help on the internet, email or via any other method of communication.

-   The exam is open-book, open-note, so you may use any materials from class as you take the exam.

## Submission

-   Your answers should be typed in the document below (or answer by deleting alternative answers in multiple choice questions.

-   Make sure you **commit** any changes and **push** the changes to the course repository before the end of the exam.

-   Once the final exam has ended, the contents of your repository will be pulled for grading. This will happen only once, so no changes made after the end of the exam will be recorded.

------------------------------------------------------------------------

# Part 1

## Q-1

In the context of time series, ***partial autocorrelation*** measures:

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

*Delete the wrong answer(s):*

-   The direct effect of past values on the current value
-   The total correlation between two points in time
-   The indirect effect of past values on the current value
-   The correlation between two variables, removing the effect of intervening variables
:::

## Q-2

In a causal DAG, a ***confounder*** is:

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

*Delete the wrong answer(s) below*.

-   A variable that influences both the cause and effect
-   A variable that is affected by the cause
-   A variable that has no impact on the relationship
-   A variable that is only affected by the effect
:::

## Q-3

***Stationarity*** in time series analysis means that:

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

*Delete the wrong answer(s) below*.

-   The series has no missing values
-   The series is increasing over time
-   The series has a constant mean and variance over time
-   The series is represented by a straight line
:::

## Q-4

For the binary classifier with the confusion matrix below:

![](images/binary_confusion.png){fig-align="center" width="250"}

```{r}
# 
# tibble::tibble(r = c('data = 1','data = 0'), 'predict = 1' = c(45,15), 'predict = 0' = c(8,32)) %>% 
#   gt::gt('r') %>% 
#   gt::tab_header(title = 'Confusion matrix', subtitle = 'binary classes') %>% 
#   gtExtras::gt_theme_espn() %>% 
#   gt::gtsave('images/binary_confusion.png')

(45)/(45+16)
(45)/(45+8)
(45)/(45+32)
(45+32)/(45+8+15+32)

  
```

The ***precision*** of this binary classifier is approximately:

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

Delete the wrong answer(s) below:

-   0.11
-   0.85
-   0.74
-   0.58
-   0.77
:::

## Q-5

Which distance metric is not commonly used in kNN classifiers?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

Keep the answer(s) you think might be appropriate

-   Euclidean distance
-   Manhattan distance
-   Cosine similarity
-   Minkowski distance
:::

## Q6

In causal DAGs, what does a directed edge represent?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

Delete the wrong answer(s) below

-   Correlation
-   Causation
-   Similarity
-   Distance
:::

## Q7

How does the kNN algorithm typically perform on very large datasets?:

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

Delete the wrong answer(s) below

-   It becomes faster as it has more data points to search
-   It becomes slower due to the increased computation of distances
-   Its performance does not depend on the size of the dataset
-   It automatically reduces the dimensionality of the data
:::

## Q8

Can kNN be used for regression tasks?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

Delete the wrong answer(s) below

-   No, because kNN cannot handle continuous data
-   Yes, but only with a special variant of kNN
-   No, kNN is strictly a classification algorithm
-   Yes, by taking the mean or median of the neighbors' values
:::

## Q9

What is the purpose of introducing a soft margin in a SVM?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

Delete the wrong answer(s) below

-   To ensure that the SVM can only be used for linearly separable data
-   To reduce the dimensionality of the feature space
-   To allow for a certain degree of misclassification in the training data
-   To increase the computational efficiency of the model
:::

## Q10

SVM vs Logistic Regression: In what scenario might an SVM be preferred over logistic regression?

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER (1 point) :

Delete the wrong answer(s) below

-   When the dataset is very large
-   When the classes are well-separated
-   When interpretability of the model is a key requirement
-   When the dataset contains a high number of categorical features
:::

# Part 2

## Q9

For the data below, it is expected that the response variable y can be described by the independent variables x1 and x2. This implies that the parameters of the following model should be estimated and tested

$$
y = \beta_0 + \beta_1x1 + \beta_2x2 + \epsilon, \epsilon âˆ¼ \mathcal{N}(0, \sigma^2)
$$

```{r}
dat <- tibble::tibble(
  x1=c(0.58, 0.86, 0.29, 0.20, 0.56, 0.28, 0.08, 0.41, 0.22, 0.35, 0.59, 0.22, 0.26, 0.12, 0.65, 0.70, 0.30
        , 0.70, 0.39, 0.72, 0.45, 0.81, 0.04, 0.20, 0.95)
  , x2=c(0.71, 0.13, 0.79, 0.20, 0.56, 0.92, 0.01, 0.60, 0.70, 0.73, 0.13, 0.96, 0.27, 0.21, 0.88, 0.30
        , 0.15, 0.09, 0.17, 0.25, 0.30, 0.32, 0.82, 0.98, 0.00)
  , y=c(1.45, 1.93, 0.81, 0.61, 1.55, 0.95, 0.45, 1.14, 0.74, 0.98, 1.41, 0.81, 0.89, 0.68, 1.39, 1.53
        , 0.91, 1.49, 1.38, 1.73, 1.11, 1.68, 0.66, 0.69, 1.98)
)
```

Calculate the parameter estimates ( $\hat{\beta}_0$, $\hat{\beta}_1$, and $\hat{\beta}_2$); in addition find the usual 95% confidence intervals for $\beta_0$, $\beta_1$, $\beta_2$.

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# your code goes here
```

Hint: use `broom::tidy(conf.int = TRUE)` with a regression model
:::

Bonus: using the `.resid` column created by `broom::augment(___, dat)` , calculate $\hat{\sigma}^2$.

## Q10

Execute the following code to read sales data from a csv file and answer the questions about the code below.

```{r}
#| echo: true
#| message: false
#| error: false

require(magrittr)
require(ggplot2)

# read sales data
sales_dat <-
  readr::read_csv("data/sales_data_sample.csv", show_col_types = FALSE) %>%
  janitor::clean_names() %>% 
  dplyr::mutate(
    orderdate = lubridate::as_date(orderdate, format = "%m/%d/%Y %H:%M")
    , orderdate = lubridate::year(orderdate)
  )

```

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER :

Without executing the code:

-   describe the result of the **`clean_names`** step.
-   describe the result of the **`mutate`** step.
:::

## Q11

Using the sales data that was loaded in the last question, describe what the `group_by` step does in the code below, and complete the code to produce a sales summary by year, i.e. `productline` and years are the columns (one column for each year), while each year column contains the sales for each productline that year. Execute your code to confirm that it is doing what you expect.

```{r}
#| eval: false
#| message: false
  sales_dat %>% 
    dplyr::group_by(orderdate, productline) %>% 
    dplyr::summarize( sales = sum(___) ) %>% 
    tidyr::pivot_wider(names_from = ___, values_from = ___)
```

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER :

-   the result of the **`group_by`** step is:

-   the sales summary table produced by the code is given below

```{r}
# executed code

```
:::

## Q12

Execute the following code to read salary data from a csv file and perform the following analyses.

-   code an ordinary linear regression to estimate the best linear relationship between monthly salary (the outcome, in \$000's) and months of experience.
-   extract the residuals from the fit object( using fit\$residuals) and show the qqplot (using `qqnorm`) for the residuals.
-   which assumption of ordinary linear regression does the qqplot validate.
-   is the assumption satisfied in this case?

```{r}
#| echo: true
salary_dat <-
  readr::read_csv("data/Experience-Salary.csv", show_col_types = FALSE) %>%
  janitor::clean_names()
```

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}

```
:::

## Q13

Execute the following code to read student performance data from a csv file and code the following analyses where the outcome/target variable is **performance_index**, and all other columns are predictors.

-   use `rsample::initial_split` to create training and test datasets, and extract the training and test datasets using the corresponding `rsample::?` functions
-   use `recipes::recipe` to preprocess the data by normalizing the numeric predictors and creating dummy variables for the nominal predictors
-   prep the recipe you created and then use it with recipes::bake applied to the **training** dataset to create a analysis dataset.
-   run an ordinary linear regression to predict **performance_index** from the predictors, using the analysis dataset. Save the fit object.
-   use `broom::augment` (fit, data) to combine your linear fit with the corresponding training data. This step gives a tibble with a `.resid` column that is the difference between the prediction and the observed value.
-   use the .resid column to calculate the mean squared error (mse) of the fit to the training data.

```{r}
#| echo: true
#| eval: false
performance_dat <-
  readr::read_csv("data/Student_Performance.csv", show_col_types = FALSE) %>%
  janitor::clean_names()
```

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
#| eval: false
set.seed(8740)
splits <- rsample::initial_split(performance_dat)
training <- ?
testing <- ?

rec <- training %>%
  recipes::recipe(performance_index ~ .) %>% 
  recipes::step_ ?  %>% 
  recipes::setp_ ?
  
performance_fit <- lm(performance_index ~ .,
   data = ?)  

result <- broom::augment(?,?)

result %>% 
  dplyr::mutate( mse = ?)
```
:::

## Q14

Execute the following code to read employee absence data from a *.xls* file and create a recipe to preprocess the data. Do the following:

-   familiarize yourself with the data using `skimr::skim()` ,
-   describe what each step of the recipe is doing,
-   split the data into training and test sets, where the training set represents 80% of the data (the default is 75%).
-   add the missing arguments to the code that trains an xgboost model. You will need to prep the recipe & bake the prepped recipe. Finally you will need to ensure your argument is a matrix, by using `as.matrix()` .
-   pull the top 10 predictors from the model using `xgboost::xgb.importance(model = ., top_n = 10)`
-   finally, plot the top 10 predictors. What it the most important predictor of absenteeism time per this model?

```{r}
#| eval: false
dat <-
  readxl::read_xls("data/Absenteeism_at_work.xls") %>%
  janitor::clean_names() %>%
  # drop id, because it has no predictive value
  # drop reason_for_absence because it has too much predictive value
  dplyr::select( -c(id, reason_for_absence) )

absenteeism_rec <- dat %>%
  recipes::recipe(absenteeism_time_in_hours ~ .) %>%
  recipes::step_mutate(
    month_of_absence   = ifelse(month_of_absence>0, month.name[month_of_absence], "unknown")
    , day_of_the_week  = lubridate::wday(day_of_the_week, label=T)
    , seasons          = dplyr::case_when(
                            seasons==1 ~ 'winter',seasons==2 ~ 'spring'
                            ,seasons==3 ~ 'summer',seasons==4 ~ 'fall'
                          )
    , social_drinker   = dplyr::case_when(social_drinker>0 ~ "Yes", TRUE ~ "No")
    , social_smoker    = dplyr::case_when(social_smoker>0 ~ "Yes", TRUE ~ "No")
    , disciplinary_failure = dplyr::case_when(disciplinary_failure>0 ~ "Yes", TRUE ~ "No")
  ) %>%
  recipes::step_string2factor(where(is.character)) %>%
  recipes::step_normalize(recipes::all_numeric_predictors()) %>%
  recipes::step_dummy(recipes::all_nominal_predictors())

untuned_xgb <-
  xgboost::xgboost(
    data = ?,
    label = ?,
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
    , verbose = FALSE
  )
```

::: {.callout-note appearance="simple" icon="false"}
## YOUR ANSWER:

```{r}
# your code goes here
```
:::

# Grading (20 pts)

| **Part**                | **Points** |
|:------------------------|:----------:|
| **Part 1 - Conceptual** |     10     |
| **Part 2 - Applied**    |     15     |
| **Total**               |     25     |
